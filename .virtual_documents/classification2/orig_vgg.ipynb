import torch
from torch import nn
from torchvision import transforms
from torchvision import models
import matplotlib.pyplot as plt
from PIL import Image
from PIL import ImageFilter
from PIL import ImageOps
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
from copy import copy
from copy import deepcopy
import sys
import os
import pandas as pd
import numpy as np
import random
from pathlib import Path

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' # dead kernel for matplotlib


device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)


metadata = pd.read_csv('../doby_meta.csv')
metadata.info()


metadata = metadata[metadata['subject_id'] < 16000000]
metadata.info()


SEG_BASE_PATH = '../chest-x-ray-dataset-with-lung-segmentation-1.0.0/chest-x-ray-dataset-with-lung-segmentation-1.0.0'
ORIG_BASE_PATH = '../physionet.org/files/mimic-cxr-jpg/2.0.0'


class Resize(object):
    def __init__(self, size):
        self.size = size

    def __call__(self, x_seg, x_orig):
        return x_seg, x_orig.resize((self.size, self.size))

class MixImage(object):
    def __call__(self, fore, back):
        back.paste(fore, (0, 0), fore)
        return back

TRANSFORMS = transforms.Compose([
    transforms.ToTensor()
])


class Dataset(Dataset):
    def __init__(self, metadata, orig_base_path, transform=None):
        self.metadata = metadata
        self.orig_base_path = Path(orig_base_path)
        self.transform = transform

    def __getitem__(self, idx):
        x_path = self.metadata.loc[idx, 'DicomPath']
        x_orig_path = self.orig_base_path / Path(x_path)
        
        x_orig = Image.open(x_orig_path).convert('L').resize((64, 64))

        # x_orig = x_orig.filter(ImageFilter.GaussianBlur(2))

        x_orig = ImageOps.equalize(x_orig)
        
        y = self.metadata.loc[idx, 'normal']

        if self.transform:
            x = self.transform(x_orig)

        return x, y

    def __len__(self):
        return self.metadata['normal'].count()


ds = Dataset(metadata, ORIG_BASE_PATH, transform=TRANSFORMS)


from torchvision.transforms.functional import to_pil_image

x, y = ds[201]
plt.title(y)
plt.imshow(to_pil_image(x), cmap='gray')
plt.axis('off')
plt.show()


ds_size = len(ds)
train_size = int(ds_size * 0.8)
test_size = ds_size - train_size
train_ds, test_ds = random_split(ds, [train_size, test_size], generator=torch.manual_seed(42))


print(len(train_ds), len(test_ds))


train_ds = copy(train_ds)

TRAIN_TRANSFORM = transforms.Compose([
    transforms.ToTensor(),
    # rotation degree = -10, 10
    # translate -img_width * a < dx < img_width * a
        # -11.2 < dx < 11.2, y도 b로 마찬가지, tuple 형태로
    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05))
])

train_ds.dataset.transform = TRAIN_TRANSFORM


train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)
test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)


class CusVgg16(nn.Module):
    def __init__(self):
        super(CusVgg16, self).__init__()
        self.vgg = models.vgg16(weights='IMAGENET1K_V1')
        self.vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)
        self.vgg.classifier[6] = nn.Linear(4096, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.vgg(x)
        x = self.sigmoid(x)

        return x


device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CusVgg16()
model.to(device)


i = 0
for name, param in model.named_parameters():
    print(i, name)
    i += 1


i = 0
for name, param in model.named_parameters():
    if i == 0 or i == 1 or i == 30 or i == 31:
        print('requires_grad = True')
    else:
        param.requires_grad = False
    i += 1


print(model.vgg.features[0].weight.requires_grad)
print(model.vgg.features[0].bias.requires_grad)
print(model.vgg.classifier[6].weight.requires_grad)
print(model.vgg.classifier[6].bias.requires_grad)
print(model.vgg.classifier[3].bias.requires_grad)


optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
schedular = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)
weights = (metadata['normal'] == 1).sum() / (metadata['normal'] == 0).sum()
loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([weights])).to(device)


def train(model, data_loader, optimizer, epoch):
    model.train()
    running_loss = 0
    running_acc = 0
    n_data = 0

    for batch_idx, (batch, target) in enumerate(data_loader, start=1):
        batch, target = batch.to(device), target.to(device)

        optimizer.zero_grad()

        output = model(batch)
        # print(output.shape, target.shape)
        target_ = target
        target = target.unsqueeze(dim=-1).float()

        loss = loss_fn(output, target)
        running_loss += loss.item()

        predicted = (output >= torch.FloatTensor([0.5]).to(device)).type(torch.float32)
        correct = (predicted == target).sum().item()
        running_acc += correct

        loss.backward()
        optimizer.step()

        n_data += len(batch)
        print(f'\rTrain Epoch: {epoch} [{n_data}/{len(data_loader.dataset)} ({100 * batch_idx / len(data_loader):.2f}%)]  Accuracy: {100*running_acc/n_data:.2f}%  Loss: {running_loss/batch_idx:.4f}', end='')


def test(model, data_loader):
    model.eval()
    test_acc = 0
    test_loss = 0
    n_data = 0
    TP, FP, TN, FN = 0, 0, 0, 0
    with torch.no_grad():
        for batch, target in data_loader:
            batch, target = batch.to(device), target.to(device)

            output = model(batch)
            target = target.unsqueeze(dim=-1).float()

            loss = loss_fn(output, target)
            test_loss += loss.item()

            predicted = (output >= torch.FloatTensor([0.5]).to(device)).type(torch.float32)
            correct = (predicted == target).sum().item()
            test_acc += correct

            TP += ((predicted == target) & (target == 1)).sum().item()
            FP += ((predicted != target) & (target == 0)).sum().item()
            TN += ((predicted == target) & (target == 0)).sum().item()
            FN += ((predicted != target) & (target == 1)).sum().item()
            
            n_data += len(batch)
            print(f'\rTest set: [{100*n_data/len(data_loader.dataset):.2f}%]', end='')
    
    test_acc = 100 * test_acc / len(data_loader.dataset)
    test_loss = test_loss / len(data_loader)
    
    print(f'\rTest set: Accuracy: {test_acc:.2f}%  Loss: {test_loss:.4f}')

    return test_acc, test_loss, TP, FP, TN, FN


def getMetric(TP, FP, TN, FN):
    # base case: divide by zero
    TP = 0.1 if TP == 0 else TP
    FP = 0.1 if FP == 0 else FP
    TN = 0.1 if TN == 0 else TN
    FN = 0.1 if FN == 0 else FN
    
    sensitivity = TP/(TP+FN)
    specificity = TN/(TN+FP)
    precision = TP/(TP+FP)
    recall = TP/(TP+FN)
    f1_score = 2*precision*recall/(precision+recall)
    return sensitivity, specificity, f1_score


accs = []
losses = []
best_acc = 0
best_f1 = 0

best_acc_model = None
best_acc_model_state = None
best_f1_model = None
best_f1_model_state = None


for epoch in range(1, 30+1):
    train(model, train_dl, optimizer, epoch)
    
    print()
    
    acc, loss, tp, fp, tn, fn = test(model, test_dl)
    sensitivity, specificity, f1_score = getMetric(tp, fp, tn, fn)
    print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')
    print(f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, F1-Score: {f1_score:.4f}')

    if acc > best_acc:
        best_acc = acc
        best_acc_model = deepcopy(model)
        best_acc_model_state = deepcopy(model.state_dict())

    if f1_score > best_f1:
        best_f1 = f1_score
        best_f1_model = deepcopy(model)
        best_f1_model_state = deepcopy(model.state_dict())
        
    schedular.step(loss)
    accs.append(acc)
    losses.append(loss)

    print('================================================================')


accs_np = np.array(accs)
losses_np = np.array(losses)
np.save('./parameters/orig_acc.npy', accs_np)
np.save('./parameters/orig_loss.npy', losses_np)

torch.save(best_acc_model, './parameters/orig_best_acc_model.pt')
torch.save(best_acc_model_state, './parameters/orig_best_acc_model_state.pt')
torch.save(best_f1_model, './parameters/orig_best_f1_model.pt')
torch.save(best_f1_model_state, './parameters/orig_best_f1_model_state.pt')
