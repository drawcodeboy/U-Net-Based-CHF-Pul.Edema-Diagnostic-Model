{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f5fa7a-9d16-4a0a-856a-1e53edfccd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "from PIL import ImageOps\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' # dead kernel for matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cd9a73-9ac3-4019-ac0b-d2ec3933ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061a30c3-4dc7-4a57-94fc-ca622f3dc81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7064 entries, 0 to 7063\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   subject_id      7064 non-null   int64 \n",
      " 1   study_id        7064 non-null   int64 \n",
      " 2   dicom_id        7064 non-null   object\n",
      " 3   DicomPath       7064 non-null   object\n",
      " 4   edema_severity  7064 non-null   int64 \n",
      " 5   normal          7064 non-null   int64 \n",
      " 6   CHF             7064 non-null   bool  \n",
      "dtypes: bool(1), int64(4), object(2)\n",
      "memory usage: 338.2+ KB\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('../doby_meta.csv')\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d624f712-7c99-4307-a69e-ec6fed7f8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4198 entries, 0 to 4197\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   subject_id      4198 non-null   int64 \n",
      " 1   study_id        4198 non-null   int64 \n",
      " 2   dicom_id        4198 non-null   object\n",
      " 3   DicomPath       4198 non-null   object\n",
      " 4   edema_severity  4198 non-null   int64 \n",
      " 5   normal          4198 non-null   int64 \n",
      " 6   CHF             4198 non-null   bool  \n",
      "dtypes: bool(1), int64(4), object(2)\n",
      "memory usage: 233.7+ KB\n"
     ]
    }
   ],
   "source": [
    "metadata = metadata[metadata['subject_id'] < 16000000]\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b82733-7fb6-4568-92a7-6435c77b66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_BASE_PATH = '../chest-x-ray-dataset-with-lung-segmentation-1.0.0/chest-x-ray-dataset-with-lung-segmentation-1.0.0'\n",
    "ORIG_BASE_PATH = '../physionet.org/files/mimic-cxr-jpg/2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4786dc4d-739e-4a39-b3a3-3637ca8cc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, x_seg, x_orig):\n",
    "        return x_seg, x_orig.resize((self.size, self.size))\n",
    "\n",
    "class MixImage(object):\n",
    "    def __call__(self, fore, back):\n",
    "        back.paste(fore, (0, 0), fore)\n",
    "        return back\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97f46e9-ab63-4b8a-9f96-00a78d30ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, metadata, orig_base_path, transform=None):\n",
    "        self.metadata = metadata\n",
    "        self.orig_base_path = Path(orig_base_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_path = self.metadata.loc[idx, 'DicomPath']\n",
    "        x_orig_path = self.orig_base_path / Path(x_path)\n",
    "        \n",
    "        x_orig = Image.open(x_orig_path).convert('L').resize((64, 64))\n",
    "\n",
    "        x_orig = x_orig.filter(ImageFilter.GaussianBlur(1))\n",
    "\n",
    "        x_orig = ImageOps.equalize(x_orig)\n",
    "        \n",
    "        y = self.metadata.loc[idx, 'normal']\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x_orig)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata['normal'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8672ef33-1416-45cb-8579-9c9497ffdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(metadata, ORIG_BASE_PATH, transform=TRANSFORMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49c3500-86ba-482a-b16a-c0037f9a125f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "x, y = ds[201]\n",
    "plt.title(y)\n",
    "plt.imshow(to_pil_image(x), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dccc0d0f-82cd-4ae8-8f59-64b51ca84d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = len(ds)\n",
    "train_size = int(ds_size * 0.8)\n",
    "test_size = ds_size - train_size\n",
    "train_ds, test_ds = random_split(ds, [train_size, test_size], generator=torch.manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20021866-e510-453f-a554-9c02e122add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3358 840\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13db3218-9ad0-40a4-9887-03263d78c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = copy(train_ds)\n",
    "\n",
    "TRAIN_TRANSFORM = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # rotation degree = -10, 10\n",
    "    # translate -img_width * a < dx < img_width * a\n",
    "        # -11.2 < dx < 11.2, y도 b로 마찬가지, tuple 형태로\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05))\n",
    "])\n",
    "\n",
    "train_ds.dataset.transform = TRAIN_TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d99fd2-6239-421e-ac84-875237195df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72bdca3-2427-47a9-a4ef-76475ca754d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CusVgg16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CusVgg16, self).__init__()\n",
    "        self.vgg = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        self.vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.vgg.classifier[6] = nn.Linear(4096, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91cd15e7-2057-4d6f-ba80-672ac2e7a7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CusVgg16(\n",
       "  (vgg): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CusVgg16()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1993a2e1-786f-44e2-926c-ec1d313a4c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vgg.features.0.weight\n",
      "1 vgg.features.0.bias\n",
      "2 vgg.features.2.weight\n",
      "3 vgg.features.2.bias\n",
      "4 vgg.features.5.weight\n",
      "5 vgg.features.5.bias\n",
      "6 vgg.features.7.weight\n",
      "7 vgg.features.7.bias\n",
      "8 vgg.features.10.weight\n",
      "9 vgg.features.10.bias\n",
      "10 vgg.features.12.weight\n",
      "11 vgg.features.12.bias\n",
      "12 vgg.features.14.weight\n",
      "13 vgg.features.14.bias\n",
      "14 vgg.features.17.weight\n",
      "15 vgg.features.17.bias\n",
      "16 vgg.features.19.weight\n",
      "17 vgg.features.19.bias\n",
      "18 vgg.features.21.weight\n",
      "19 vgg.features.21.bias\n",
      "20 vgg.features.24.weight\n",
      "21 vgg.features.24.bias\n",
      "22 vgg.features.26.weight\n",
      "23 vgg.features.26.bias\n",
      "24 vgg.features.28.weight\n",
      "25 vgg.features.28.bias\n",
      "26 vgg.classifier.0.weight\n",
      "27 vgg.classifier.0.bias\n",
      "28 vgg.classifier.3.weight\n",
      "29 vgg.classifier.3.bias\n",
      "30 vgg.classifier.6.weight\n",
      "31 vgg.classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(i, name)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1cea3c0-d278-47d6-92ec-fcb3921110bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad = True\n",
      "requires_grad = True\n",
      "requires_grad = True\n",
      "requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if i == 0 or i == 1 or i == 30 or i == 31:\n",
    "        print('requires_grad = True')\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae412031-e271-4592-801e-6cfc43d9d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model.vgg.features[0].weight.requires_grad)\n",
    "print(model.vgg.features[0].bias.requires_grad)\n",
    "print(model.vgg.classifier[6].weight.requires_grad)\n",
    "print(model.vgg.classifier[6].bias.requires_grad)\n",
    "print(model.vgg.classifier[3].bias.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c53211a-19ab-4a4c-8dbd-cdc661104c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "schedular = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "weights = (metadata['normal'] == 1).sum() / (metadata['normal'] == 0).sum()\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([weights])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "120cb532-d591-43d2-8eeb-14dcbd8a5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    n_data = 0\n",
    "\n",
    "    for batch_idx, (batch, target) in enumerate(data_loader, start=1):\n",
    "        batch, target = batch.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(batch)\n",
    "        # print(output.shape, target.shape)\n",
    "        target_ = target\n",
    "        target = target.unsqueeze(dim=-1).float()\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predicted = (output >= torch.FloatTensor([0.5]).to(device)).type(torch.float32)\n",
    "        correct = (predicted == target).sum().item()\n",
    "        running_acc += correct\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_data += len(batch)\n",
    "        print(f'\\rTrain Epoch: {epoch} [{n_data}/{len(data_loader.dataset)} ({100 * batch_idx / len(data_loader):.2f}%)]  Accuracy: {100*running_acc/n_data:.2f}%  Loss: {running_loss/batch_idx:.4f}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee23cd11-d862-4790-b640-a0703679befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "    n_data = 0\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, target in data_loader:\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "\n",
    "            output = model(batch)\n",
    "            target = target.unsqueeze(dim=-1).float()\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predicted = (output >= torch.FloatTensor([0.5]).to(device)).type(torch.float32)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            test_acc += correct\n",
    "\n",
    "            TP += ((predicted == target) & (target == 1)).sum().item()\n",
    "            FP += ((predicted != target) & (target == 0)).sum().item()\n",
    "            TN += ((predicted == target) & (target == 0)).sum().item()\n",
    "            FN += ((predicted != target) & (target == 1)).sum().item()\n",
    "            \n",
    "            n_data += len(batch)\n",
    "            print(f'\\rTest set: [{100*n_data/len(data_loader.dataset):.2f}%]', end='')\n",
    "    \n",
    "    test_acc = 100 * test_acc / len(data_loader.dataset)\n",
    "    test_loss = test_loss / len(data_loader)\n",
    "    \n",
    "    print(f'\\rTest set: Accuracy: {test_acc:.2f}%  Loss: {test_loss:.4f}')\n",
    "\n",
    "    return test_acc, test_loss, TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8815f35-6314-4a99-b1cc-d2901e071ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetric(TP, FP, TN, FN):\n",
    "    # base case: divide by zero\n",
    "    TP = 0.1 if TP == 0 else TP\n",
    "    FP = 0.1 if FP == 0 else FP\n",
    "    TN = 0.1 if TN == 0 else TN\n",
    "    FN = 0.1 if FN == 0 else FN\n",
    "    \n",
    "    sensitivity = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return sensitivity, specificity, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563014d3-2b4e-4b3c-86a3-4102196b1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "losses = []\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "\n",
    "best_acc_model = None\n",
    "best_acc_model_state = None\n",
    "best_f1_model = None\n",
    "best_f1_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d47b8dc2-72d1-41a9-9d01-6083a753bbe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3358/3358 (100.00%)]  Accuracy: 59.44%  Loss: 0.7812\n",
      "Test set: Accuracy: 59.29%  Loss: 0.7688\n",
      "TP: 339, FP: 180, TN: 159, FN: 162\n",
      "Sensitivity: 0.6766, Specificity: 0.4690, F1-Score: 0.6647\n",
      "================================================================\n",
      "Train Epoch: 2 [3358/3358 (100.00%)]  Accuracy: 61.02%  Loss: 0.7688\n",
      "Test set: Accuracy: 60.24%  Loss: 0.7705\n",
      "TP: 364, FP: 197, TN: 142, FN: 137\n",
      "Sensitivity: 0.7265, Specificity: 0.4189, F1-Score: 0.6855\n",
      "================================================================\n",
      "Train Epoch: 3 [3358/3358 (100.00%)]  Accuracy: 62.51%  Loss: 0.7635\n",
      "Test set: Accuracy: 56.55%  Loss: 0.7771\n",
      "TP: 241, FP: 105, TN: 234, FN: 260\n",
      "Sensitivity: 0.4810, Specificity: 0.6903, F1-Score: 0.5691\n",
      "================================================================\n",
      "Train Epoch: 4 [3358/3358 (100.00%)]  Accuracy: 60.15%  Loss: 0.7692\n",
      "Test set: Accuracy: 62.02%  Loss: 0.7608\n",
      "TP: 355, FP: 173, TN: 166, FN: 146\n",
      "Sensitivity: 0.7086, Specificity: 0.4897, F1-Score: 0.6900\n",
      "================================================================\n",
      "Train Epoch: 5 [3358/3358 (100.00%)]  Accuracy: 61.23%  Loss: 0.7635\n",
      "Test set: Accuracy: 61.07%  Loss: 0.7606\n",
      "TP: 351, FP: 177, TN: 162, FN: 150\n",
      "Sensitivity: 0.7006, Specificity: 0.4779, F1-Score: 0.6822\n",
      "================================================================\n",
      "Train Epoch: 6 [3358/3358 (100.00%)]  Accuracy: 61.38%  Loss: 0.7647\n",
      "Test set: Accuracy: 63.10%  Loss: 0.7661\n",
      "TP: 462, FP: 271, TN: 68, FN: 39\n",
      "Sensitivity: 0.9222, Specificity: 0.2006, F1-Score: 0.7488\n",
      "================================================================\n",
      "Train Epoch: 7 [3358/3358 (100.00%)]  Accuracy: 61.17%  Loss: 0.7651\n",
      "Test set: Accuracy: 59.40%  Loss: 0.7662\n",
      "TP: 307, FP: 147, TN: 192, FN: 194\n",
      "Sensitivity: 0.6128, Specificity: 0.5664, F1-Score: 0.6429\n",
      "================================================================\n",
      "Train Epoch: 8 [3358/3358 (100.00%)]  Accuracy: 62.30%  Loss: 0.7611\n",
      "Test set: Accuracy: 60.24%  Loss: 0.7643\n",
      "TP: 375, FP: 208, TN: 131, FN: 126\n",
      "Sensitivity: 0.7485, Specificity: 0.3864, F1-Score: 0.6919\n",
      "================================================================\n",
      "Train Epoch: 9 [3358/3358 (100.00%)]  Accuracy: 62.75%  Loss: 0.7613\n",
      "Test set: Accuracy: 61.55%  Loss: 0.7607\n",
      "TP: 357, FP: 179, TN: 160, FN: 144\n",
      "Sensitivity: 0.7126, Specificity: 0.4720, F1-Score: 0.6885\n",
      "================================================================\n",
      "Train Epoch: 10 [3358/3358 (100.00%)]  Accuracy: 61.79%  Loss: 0.7626\n",
      "Test set: Accuracy: 61.31%  Loss: 0.7664\n",
      "TP: 410, FP: 234, TN: 105, FN: 91\n",
      "Sensitivity: 0.8184, Specificity: 0.3097, F1-Score: 0.7162\n",
      "================================================================\n",
      "Train Epoch: 11 [3358/3358 (100.00%)]  Accuracy: 63.34%  Loss: 0.7563\n",
      "Test set: Accuracy: 60.71%  Loss: 0.7616\n",
      "TP: 274, FP: 103, TN: 236, FN: 227\n",
      "Sensitivity: 0.5469, Specificity: 0.6962, F1-Score: 0.6241\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "================================================================\n",
      "Train Epoch: 12 [3358/3358 (100.00%)]  Accuracy: 63.04%  Loss: 0.7546\n",
      "Test set: Accuracy: 60.83%  Loss: 0.7648\n",
      "TP: 333, FP: 161, TN: 178, FN: 168\n",
      "Sensitivity: 0.6647, Specificity: 0.5251, F1-Score: 0.6693\n",
      "================================================================\n",
      "Train Epoch: 13 [3358/3358 (100.00%)]  Accuracy: 63.79%  Loss: 0.7542\n",
      "Test set: Accuracy: 62.26%  Loss: 0.7630\n",
      "TP: 372, FP: 188, TN: 151, FN: 129\n",
      "Sensitivity: 0.7425, Specificity: 0.4454, F1-Score: 0.7012\n",
      "================================================================\n",
      "Train Epoch: 14 [3358/3358 (100.00%)]  Accuracy: 63.73%  Loss: 0.7522\n",
      "Test set: Accuracy: 61.79%  Loss: 0.7591\n",
      "TP: 358, FP: 178, TN: 161, FN: 143\n",
      "Sensitivity: 0.7146, Specificity: 0.4749, F1-Score: 0.6905\n",
      "================================================================\n",
      "Train Epoch: 15 [3358/3358 (100.00%)]  Accuracy: 63.16%  Loss: 0.7572\n",
      "Test set: Accuracy: 62.62%  Loss: 0.7572\n",
      "TP: 357, FP: 170, TN: 169, FN: 144\n",
      "Sensitivity: 0.7126, Specificity: 0.4985, F1-Score: 0.6946\n",
      "================================================================\n",
      "Train Epoch: 16 [3358/3358 (100.00%)]  Accuracy: 63.22%  Loss: 0.7554\n",
      "Test set: Accuracy: 61.31%  Loss: 0.7597\n",
      "TP: 338, FP: 162, TN: 177, FN: 163\n",
      "Sensitivity: 0.6747, Specificity: 0.5221, F1-Score: 0.6753\n",
      "================================================================\n",
      "Train Epoch: 17 [3358/3358 (100.00%)]  Accuracy: 63.10%  Loss: 0.7576\n",
      "Test set: Accuracy: 60.71%  Loss: 0.7626\n",
      "TP: 348, FP: 177, TN: 162, FN: 153\n",
      "Sensitivity: 0.6946, Specificity: 0.4779, F1-Score: 0.6784\n",
      "================================================================\n",
      "Train Epoch: 18 [3358/3358 (100.00%)]  Accuracy: 63.70%  Loss: 0.7548\n",
      "Test set: Accuracy: 61.43%  Loss: 0.7607\n",
      "TP: 342, FP: 165, TN: 174, FN: 159\n",
      "Sensitivity: 0.6826, Specificity: 0.5133, F1-Score: 0.6786\n",
      "================================================================\n",
      "Train Epoch: 19 [3358/3358 (100.00%)]  Accuracy: 64.95%  Loss: 0.7529\n",
      "Test set: Accuracy: 61.31%  Loss: 0.7598\n",
      "TP: 360, FP: 184, TN: 155, FN: 141\n",
      "Sensitivity: 0.7186, Specificity: 0.4572, F1-Score: 0.6890\n",
      "================================================================\n",
      "Train Epoch: 20 [3358/3358 (100.00%)]  Accuracy: 63.49%  Loss: 0.7531\n",
      "Test set: Accuracy: 61.43%  Loss: 0.7606\n",
      "TP: 342, FP: 165, TN: 174, FN: 159\n",
      "Sensitivity: 0.6826, Specificity: 0.5133, F1-Score: 0.6786\n",
      "================================================================\n",
      "Train Epoch: 21 [3358/3358 (100.00%)]  Accuracy: 63.19%  Loss: 0.7551\n",
      "Test set: Accuracy: 60.48%  Loss: 0.7624\n",
      "TP: 327, FP: 158, TN: 181, FN: 174\n",
      "Sensitivity: 0.6527, Specificity: 0.5339, F1-Score: 0.6633\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-05.\n",
      "================================================================\n",
      "Train Epoch: 22 [3358/3358 (100.00%)]  Accuracy: 63.88%  Loss: 0.7526\n",
      "Test set: Accuracy: 61.43%  Loss: 0.7614\n",
      "TP: 337, FP: 160, TN: 179, FN: 164\n",
      "Sensitivity: 0.6727, Specificity: 0.5280, F1-Score: 0.6754\n",
      "================================================================\n",
      "Train Epoch: 23 [3358/3358 (100.00%)]  Accuracy: 63.52%  Loss: 0.7539\n",
      "Test set: Accuracy: 62.38%  Loss: 0.7585\n",
      "TP: 347, FP: 162, TN: 177, FN: 154\n",
      "Sensitivity: 0.6926, Specificity: 0.5221, F1-Score: 0.6871\n",
      "================================================================\n",
      "Train Epoch: 24 [3358/3358 (100.00%)]  Accuracy: 63.25%  Loss: 0.7551\n",
      "Test set: Accuracy: 61.07%  Loss: 0.7617\n",
      "TP: 341, FP: 167, TN: 172, FN: 160\n",
      "Sensitivity: 0.6806, Specificity: 0.5074, F1-Score: 0.6759\n",
      "================================================================\n",
      "Train Epoch: 25 [3358/3358 (100.00%)]  Accuracy: 64.32%  Loss: 0.7500\n",
      "Test set: Accuracy: 61.43%  Loss: 0.7620\n",
      "TP: 343, FP: 166, TN: 173, FN: 158\n",
      "Sensitivity: 0.6846, Specificity: 0.5103, F1-Score: 0.6792\n",
      "================================================================\n",
      "Train Epoch: 26 [3358/3358 (100.00%)]  Accuracy: 63.37%  Loss: 0.7541\n",
      "Test set: Accuracy: 62.38%  Loss: 0.7563\n",
      "TP: 350, FP: 165, TN: 174, FN: 151\n",
      "Sensitivity: 0.6986, Specificity: 0.5133, F1-Score: 0.6890\n",
      "================================================================\n",
      "Train Epoch: 27 [3358/3358 (100.00%)]  Accuracy: 63.76%  Loss: 0.7529\n",
      "Test set: Accuracy: 60.71%  Loss: 0.7644\n",
      "TP: 344, FP: 173, TN: 166, FN: 157\n",
      "Sensitivity: 0.6866, Specificity: 0.4897, F1-Score: 0.6758\n",
      "================================================================\n",
      "Train Epoch: 28 [3358/3358 (100.00%)]  Accuracy: 64.06%  Loss: 0.7536\n",
      "Test set: Accuracy: 62.62%  Loss: 0.7576\n",
      "TP: 350, FP: 163, TN: 176, FN: 151\n",
      "Sensitivity: 0.6986, Specificity: 0.5192, F1-Score: 0.6903\n",
      "================================================================\n",
      "Train Epoch: 29 [3358/3358 (100.00%)]  Accuracy: 63.91%  Loss: 0.7532\n",
      "Test set: Accuracy: 60.95%  Loss: 0.7595\n",
      "TP: 350, FP: 177, TN: 162, FN: 151\n",
      "Sensitivity: 0.6986, Specificity: 0.4779, F1-Score: 0.6809\n",
      "================================================================\n",
      "Train Epoch: 30 [3358/3358 (100.00%)]  Accuracy: 63.73%  Loss: 0.7536\n",
      "Test set: Accuracy: 62.26%  Loss: 0.7555\n",
      "TP: 359, FP: 175, TN: 164, FN: 142\n",
      "Sensitivity: 0.7166, Specificity: 0.4838, F1-Score: 0.6937\n",
      "================================================================\n",
      "Train Epoch: 31 [48/3358 (1.43%)]  Accuracy: 68.75%  Loss: 0.7283"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     train(model, train_dl, optimizer, epoch)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m      6\u001b[0m     acc, loss, tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m test(model, test_dl)\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m running_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m n_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (batch, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     batch, target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m x_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDicomPath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m x_orig_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_base_path \u001b[38;5;241m/\u001b[39m Path(x_path)\n\u001b[1;32m---> 11\u001b[0m x_orig \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(x_orig_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m     13\u001b[0m x_orig \u001b[38;5;241m=\u001b[39m x_orig\u001b[38;5;241m.\u001b[39mfilter(ImageFilter\u001b[38;5;241m.\u001b[39mGaussianBlur(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     15\u001b[0m x_orig \u001b[38;5;241m=\u001b[39m ImageOps\u001b[38;5;241m.\u001b[39mequalize(x_orig)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\PIL\\Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    865\u001b[0m ):\n\u001b[0;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\PIL\\ImageFile.py:249\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m         s \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodermaxblock)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\doby\\Lib\\site-packages\\PIL\\JpegImagePlugin.py:406\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes):\n\u001b[0;32m    401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(read_bytes)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ended\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50+1):\n",
    "    train(model, train_dl, optimizer, epoch)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    acc, loss, tp, fp, tn, fn = test(model, test_dl)\n",
    "    sensitivity, specificity, f1_score = getMetric(tp, fp, tn, fn)\n",
    "    print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
    "    print(f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, F1-Score: {f1_score:.4f}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_acc_model = deepcopy(model)\n",
    "        best_acc_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "    if f1_score > best_f1:\n",
    "        best_f1 = f1_score\n",
    "        best_f1_model = deepcopy(model)\n",
    "        best_f1_model_state = deepcopy(model.state_dict())\n",
    "        \n",
    "    schedular.step(loss)\n",
    "    accs.append(acc)\n",
    "    losses.append(loss)\n",
    "\n",
    "    print('================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0ecc1aa-4bb1-4a2f-afec-574e8e654326",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_np = np.array(accs)\n",
    "losses_np = np.array(losses)\n",
    "np.save('./parameters/orig_t_acc.npy', accs_np)\n",
    "np.save('./parameters/orig_t_loss.npy', losses_np)\n",
    "\n",
    "torch.save(best_acc_model, './parameters/orig_t_best_acc_model.pt')\n",
    "torch.save(best_acc_model_state, './parameters/orig_t_best_acc_model_state.pt')\n",
    "torch.save(best_f1_model, './parameters/orig_t_best_f1_model.pt')\n",
    "torch.save(best_f1_model_state, './parameters/orig_t_best_f1_model_state.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
